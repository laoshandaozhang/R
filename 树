rm(list=ls())
library(ISLR) # contains the dataset
library(tree) # contains... tree-building methods
#View(Carseats)
set.seed(4061)

# Recode response variable so as to make it a classification problem
High = ifelse(Carseats$Sales<=8, "No", "Yes")
# Create a data frame that includes both the predictors and response
# (a data frame is like an Excel spreadsheet, if you like)
CS = data.frame(Carseats, High)
CS$Sales = NULL
CS$High = as.factor(CS$High) # <-- this bit was missing

#divide into training set and testing set
train=sample(1:nrow(CS), 200)
CS.test=CS[-train,]
High.test=High[-train]

# Fit the tree using all predictors (except for variable Sales, 
# which we have "recoded" into a cateorical response variable)
# to response variable High
# tree.out = tree(High~., CS) 
# high is y, dataset=CS
tree.out = tree(High~., CS,subset=train) #fit model
#subset-> use training set to train and use testing set to test
summary(tree.out)#here the Misclassification error rate is for train not for test
# plot the tree
plot(tree.out)
text(tree.out, pretty=0)#add label

#calculate test Missclassification error rate
tree.pred=predict(tree.out,CS.test,type="class")
table(tree.pred,High.test)#confusion matrix
print(paste("The test MSE is ", (22+28)/200))# this is for test


#using 10-cv to find best tuning parameter:size of tree and penalty factor alpha(chengfaxishu)
set.seed(90)
cv.carseats=cv.tree(tree.out,FUN=prune.misclass)#tuning
#for regression tree, FUN is not need
names(cv.carseats)
#size:|T|  k:alpha
cv.carseats
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")      
#first to find the minimum in the k, then find the corresponding size
#the best is the value of size

#prune the tree
prune.carseats=prune.misclass(tree.out,best=11)
plot(prune.carseats)
text(prune.carseats,pretty=0)

#calculate test MSE
tree.pred=predict(prune.carseats,CS.test,type="class")
table(tree.pred,High.test)
print(paste("The test MSE is ", (20+26)/200))


##################regression tree####################
rm(list=ls())
library(ISLR) # contains the dataset
library(tree) # contains... tree-building methods
set.seed(4061)
#________________
#regression tree
library(MASS)
#medv is y
#View(Boston)

#training set and testing set
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)

#calcaulate test MSE
#only regression tree is MSE, classification tree is misclassification error rate
pred = predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
mean((pred-boston.test)^2)

#using 10-cv to find best tuning parameter:size of tree and penalty factor alpha(chengfaxishu)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')

#prune the tree
prune.boston=prune.tree(tree.boston,best=8)#find the minimum value in the plot
plot(prune.boston)
text(prune.boston,pretty=0)

#calculate test MSE
yhat=predict(prune.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
#if the model fit pefect, all the data are on the line
mean((yhat-boston.test)^2)
#if the MSE does not decrease
#maybe is because the dataset is too small
#prune is for reduce overfitting

#because only use one tree to predict, the effect is not very good
#so bagging, randomforest, boosting is to improve by using many trees to predict then calculate the average
#the difference between bagging and randomforest:
#bagging: use all variables to predict
#randomforest: use part of all variables to predict

# Bagging 
library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
#mtry=13 is because there are 13 variables
bag.boston

#calculate test MSE
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)

#visualize the importance plot
importance(bag.boston)
varImpPlot(bag.boston)
#in the plot, the up value is more important

#another argument is that we can change the number of tree
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
#in bagging, the more tree, the effect better
#by setting the 'ntree' number -> number of tree
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
bag.boston
mean((yhat.bag-boston.test)^2)


#________________________________
#randomforest
set.seed(1)
#mtry=6<13, increase some randomness to prevent overfitting
#the default value of mtry is:
#classification (sqrt(p) where p is number of variables in x) and regression (p/3)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)

importance(rf.boston)
varImpPlot(rf.boston)


#_____________________
#Boosting for the regression tree
#different from bagging and randomforest: if you need to bulid 1000 trees
#bagging and randomforest: build together
#boosting: bulid one by one
library(gbm)
set.seed(13)

#build the model 
#need to tuning(tiaocan): n.trees & interaction.depth
boost.boston=gbm(medv~.,data=Boston[train,],
                 distribution="gaussian",n.trees=5000,
                 interaction.depth=1) #=1: no interaction
#boosting need to define 3 parameters:
#shrinkage parameter lambda: the smaller, the better. default: 0.001
#number of trees B: not the larger, the better(different from bagging and randomforest), may lead to ovefitting
#number of splits
summary(boost.boston) #up is more important
plot(boost.boston,i="rm")

#calculate test MSE
yhat.boost=predict(boost.boston,newdata=Boston[-train,],
                   n.trees=5000)
mean((yhat.boost-Boston[-train,]$medv)^2)


#find the best number of tree B
number_of_tree = c( 5000, 10000, 15000, 20000 )
nt = length(number_of_tree)
cv.error = rep(0,4)

for( i in 1:nt ){ # for each number of cuts to test
  n_tree = number_of_tree[i] 
  boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=n_tree,interaction.depth=1)
  yhat.boost=predict(boost.boston,newdata=Boston[-train,],
                     n.trees=n_tree)
  cv.error[i] =mean((yhat.boost-Boston[-train,]$medv)^2)
}

cv.error
#not the lager the B is, the better 



bagging用于改进决策树(高方差)。用B个自助抽样训练集建立B棵回归树，再计算相应预测值的平均。得到的每棵树方差高偏差小。求平均可以减小方差。分类问题：采取多数投票，即B个预测中出现频率最高的类别作为总体预测。
树的个数B不是对bagging起决定作用的参数，即使很大也不会产生过拟合。在实践中取足够大的B值，使误差能够稳定下来。（随机森林也是）
bagging的关键是用bootstrap得到观测值的若干子集，并对他们建立相应的树。
bagging对预测准确性的提示是以牺牲解释性为代价的。
bagging可以减小方差，防止过拟合。

bagging和随机森林的最大不同在于预测变量子集的规模m。如果取m=p，二者相同。m是可用于分裂的变量个数。当许多预测变量相关时，较小的m建立随机森林通常很有效。

boosting和bagging的区别：每棵树的构建都需要用到之前生成的树中的信息。而bagging的树是相互独立的。
B值过大时，boosting会出现过拟合。
lambda控制者学习速度。如果很小，则B要很大才能获得良好的预测效果。
分裂点数d控制着复杂性。
boosting所需的树更小，因为生成一棵特定的树要考虑其他已有的树。更小的树有助于提高模型的可解释性。

random forest：实际上是部分实现了多次训练取平均的效果，每次训练得到的树学习能力都很强，每个的方差都较大，但综合起来比较小（相互抵消）。
——因为在训练每棵树时，是通过抽样样本集来实现多次训练的，而不同的训练集会有重合的情况，所以并不能认为是独立的多次训练。每颗训练得到的树之间的方差会有一定相关性，所以要取平均值。
GBDT：方差大，偏差小。树越多，学习能力越强，偏差越小。因为是层层递进的树。
Gradient Boosting Decision Tree梯度提升决策树

随机森林是Bagging的一种变形，在训练过程中加入随机属性选择。传统决策树训练过程是选择一个最优属性来进行分类划分，而随机森林是先选取一部分属性，再从中选择最优划分属性，这样会让训练过程更快，计算开销小。随机森林的思想就是每次只抽一部分特征来建模，在大量的树下确保所有的特征都会被使用，这样平均之下就会减弱不同树之间特征的高度相关性，以减小总体的方差，达到总体的最优。

Bagging和随机森林都是集成学习，由多个部分样本训练的基学习器共同投票/平均。随机森林在Bagging基础上加入了随机属性选择，一般来说随机森林的训练效率要优于bagging
