rm(list=ls())
library("ISLR")
View(College)
set.seed(4061)
#convert a typed variable to a numeric variable
data=data.matrix(College)

#PCA
#scale=TRUE is to Decentralization and standard deviation reduction
#quzhongxinhua  qubiaozhuncha
pr.out=prcomp(data,scale=TRUE)
names(pr.out)
#center is mean
#scale is standard deviation
#rotation: Projection Direction(eigen vectors)
#sdev: eigen values

biplot(pr.out)

pr.out$x[,1:3]#Down to 3 dimensions
plot(pr.out$x[,1:2])

#Proportion of Variance Explained
pr.var=pr.out$sdev^2
plot(pr.var, ylab="Variance", xlab="Principal Component",type='b',xlim = c(0,10))

#Cumulative Proportion of Variance Explained
#to see lost how much information
pve = cumsum(pr.var)/cumsum(pr.var)[length(pr.var)]
pve[13] #=0.974 -> lost 1-0.974 information
plot(pve, ylab="Cumulative Proportion of Variance Explained", xlab="Principal Component", ylim=c(0,1),type='b')

pr.out$x[,1:13]

#-------------------------------
#training set and testing set
train=(0:600)
training_set = College[train,]
test=(-train)
testing_set = College[(-train),]

#biplot
training_set = data.matrix(training_set[,-2])
pr.out=prcomp(training_set, scale=TRUE)
biplot(pr.out)

#Proportion of Variance Explained
pr.var=pr.out$sdev^2
plot(pr.var, ylab="Variance", xlab="Principal Component",type='b',xlim = c(0,10))

#Cumulative Proportion of Variance Explained
pve = cumsum(pr.var)/cumsum(pr.var)[length(pr.var)]

plot(pve, ylab="Cumulative Proportion of Variance Explained", xlab="Principal Component", ylim=c(0,1),type='b')
