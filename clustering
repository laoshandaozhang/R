rm(list=ls())
library(ISLR)
#View(NCI60)
nci.labs=NCI60$labs  
nci.data=NCI60$data
dim(nci.data)
nci.labs[1:4]
table(nci.labs)

#scale the data set
#to make mean=0 for each dimension and ??=1
sd.data=scale(nci.data)

#_______________________________________________
#hier. clustering
par(mfrow=c(1,3))
#Euclidean disdance
data.dist=dist(sd.data,method = "euclidean")

#Four ways to calculate the distance between classes
par(mfrow=c(1,1))
#default: complete  is to find the maximize
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Average Linkage", xlab="", sub="",ylab="")
#single  is to find the minimize
plot(hclust(data.dist, method="single"), labels=nci.labs,  main="Single Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="centroid"), labels=nci.labs,  main="centroid Linkage", xlab="", sub="",ylab="")

#Pick a method to cluster and pruning and visualize
hc.out=hclust(dist(sd.data),method ="complete")
hc.clusters=cutree(hc.out,4)
table(hc.clusters,nci.labs)
plot(hc.out, labels=nci.labs)
abline(h=139, col="red")

#________________________________________________
#kmeans
set.seed(2)
km.out=kmeans(sd.data, 4, nstart=20)
km.clusters=km.out$cluster
km.clusters

#compare the kmeans and clustering
table(km.clusters,hc.clusters)
#here, hc.3=km.4


#________________________________
#PCA can be used first and then reduce dimension
pr.out=prcomp(nci.data, scale=TRUE)

xhc.out=hclust(dist(pr.out$x[,1:5]))
plot(xhc.out, labels=nci.labs, main="Hier. Clust. on First Five Score Vectors")
abline(h=100, col="red")
table(cutree(xhc.out,4), nci.labs)


#Down to two dimensional then clustering
xhc.out=hclust(dist(pr.out$x[,1:2]))
plot(xhc.out, labels=nci.labs, main="Hier. Clust. on First Five Score Vectors")
abline(h=80, col="red")
xhc.clusters = cutree(xhc.out,3)#divide into 3 categories
xhc.clusters

plot(pr.out$x[,1:2], col=(xhc.clusters+1), 
     main="reduce dimension to 2 hier. Clustering Results with K=3", 
     xlab="", ylab="", pch=20, cex=2)



因为k-means有局限性：
1 一开始需要定义K值
2 对于很不规则的散点图时，k-means算法无法处理
所以引入：层次聚类
是一种bottom-up 从下到上的聚类方法
有12345五个点
(1)五个点各成一类：共五类
(2)找离1最近的点(比如为3) 则1和3成一类：共四类
(3)->三类

整个流程跑完，再想定义几类就定义几类。不需要事先设定K
